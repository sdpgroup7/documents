\section{Vision}

\subsection{Code Reuse}

We started off by taking group 3's from last year code.  We chose this group's code because it was written in Java which we were all familiar with allowing us to get to work straight away.  Their control G.U.I. was also very impressive, being clean and easy to use.  Their vision system had all the necessary elements including grabbing the vision feed, which other teams this year had more trouble with than us.

We also used the barrel distortion correction code from group 5.  Most groups who had barrel distortion correction were using a variant of openCV and so had functions that we couldn't use.  Group 3 had attempted to correct the barrel distortion but had been unsuccessful which is why we used group 5.  It was very effective working straight out of the box.

\subsection{In the Beginning}

After deciding on which code we were going to use we started to realise how unwieldy it was.  Most of the work was being done in an undocumented 700ish line method.  So first thing for us to do was to work out what was going and try and do our best to refactor it to make it more understandable.  We then saw that they were using both hsv and rgb values for thresholding, so we stripped out the hsv checking.  In group 3's code the thresholding was done by moving sliders around to find the optimal values, we changed this so that the thresholding would be based on a click, saving time and effort (this was later removed when Rado's better thesholding technique was introduced).  We then started the work of improving the accuracy and efficiency of their code (DO WE HAVE ANY STATS ON WHAT IS WAS WHEN WE STARTED?).  This included heavy refactoring, new algorithms for all aspects of the system.  All we are using currently is their code for grabbing frames, and some aspects of their G.U.I.

POSSIBLY NEEDS MORE WHY?  SHOULD WE GO INTO MORE DETAIL ON WHAT WE HAVE NOW


\subsection{Random Sampling for Centroid Calculation}

Centroiding can be strongly affected by noise, even just a few noisy pixels, this was a problem for us because the yellow in the vision feed was quite grey and testing for yellow pixels would capture parts of the robot that weren't yellow.  Applying a noise filtering algorithm was too slow, halving the frame rate, and clustering was a bit too slow for our tastes.  So James came up with a technique to give us the benefits of clustering in linear speed.  Below is an example of how the algorithm works using the yellow T as an example.
\begin{enumerate}
\item Go through frame pixel by pixel, if pixel is identified as yellow then place it one of five bins randomly.
\item Calculate centroid of all five bins.
\item If the centroid of a bin does not lie on a yellow pixel then discard it, thus removing the noisy pixels.
\item Take the mean of the remaining centroids and set this to be the centroid of the object, in this case the yellow T.
\item If all bins were discarded then keep the centroid where it was.
\end{enumerate}

The case of all the bins being discarded is a rare one, as there are very few noisy pixels compared to non noisy pixels.  This method was very fast, we only lost a couple of fps from the processing.  Using the vision testing system (when it was not well calibrated) it achieved accuracy of around 85\% but through oracle testing it was clear it was far more accurate, nearing 100\%.




